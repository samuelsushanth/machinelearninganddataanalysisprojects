{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samuelsushanth/machinelearninganddataanalysisprojects/blob/main/Practical_Prompt_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e95adcd-4611-45f5-80bb-2e832fbe6cd1",
      "metadata": {
        "id": "0e95adcd-4611-45f5-80bb-2e832fbe6cd1"
      },
      "source": [
        "## Comprehensive Guide to Prompt Engineering"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ll good"
      ],
      "metadata": {
        "id": "K4rV2-oHi3o9"
      },
      "id": "K4rV2-oHi3o9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5b90b0b3-c5ca-424c-a152-139350b44776",
      "metadata": {
        "id": "5b90b0b3-c5ca-424c-a152-139350b44776"
      },
      "source": [
        "#### Links:\n",
        "1. [PromptGuide](https://www.promptingguide.ai/techniques)\n",
        "\n",
        "2. [Anthropic](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/chain-prompts)\n",
        "\n",
        "3. [PromptHub](https://smith.langchain.com/hub)\n",
        "\n",
        "4. [OpenAI Examples](https://platform.openai.com/docs/examples)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05c5603f-2847-4758-be48-adf154f4b4e4",
      "metadata": {
        "id": "05c5603f-2847-4758-be48-adf154f4b4e4"
      },
      "source": [
        "- Prompt engineering is the process of designing and refining input prompts to guide large language models (LLMs) like GPT-3, GPT-4, and other similar AI models to perform specific tasks effectively.\n",
        "- Prompt engineering is the art and science of designing and optimizing prompts to guide AI models, particularly LLMs, towards generating the desired responses. By carefully crafting prompts, you provide the model with context, instructions, and examples that help it understand your intent and respond in a meaningful way. Think of it as providing a roadmap for the AI, steering it towards the specific output you have in mind.\n",
        "- It involves crafting the instructions and context provided to the model in a way that optimizes the accuracy and relevance of the model's output.\n",
        "- Prompt engineering is a crucial skill for leveraging the full potential of LLMs in various applications.\n",
        "#### Basics of Prompting\n",
        "- Clarity: The prompt should be clear and unambiguous. Avoid vague or overly broad requests.\n",
        "    - Example: Instead of \"Tell me about animals,\" use \"Provide an overview of the characteristics and habitats of African elephants.\"\n",
        "- Context: Providing context helps the model understand the background or setting.\n",
        "    - Example: \"In the context of a corporate workplace, explain the importance of diversity and inclusion policies.\"\n",
        "- Specificity: Be specific about what you want the model to do.\n",
        "    - Example: \"Write a five-paragraph essay on the effects of climate change on polar bears.\"\n",
        "- Instructions: Clearly state the instructions and format for the desired output.\n",
        "    - Example: \"List three benefits of renewable energy in bullet points.\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82eef131-66d2-424a-bd97-589219f4488b",
      "metadata": {
        "id": "82eef131-66d2-424a-bd97-589219f4488b"
      },
      "source": [
        "### Elements of a Prompt\n",
        "A prompt contains any of the following elements:\n",
        "\n",
        "**Instruction** - a specific task or instruction you want the model to perform\n",
        "\n",
        "**Context** - external information or additional context that can steer the model to better responses\n",
        "\n",
        "**Input Data** - the input or question that we are interested to find a response for\n",
        "\n",
        "**Output Indicator** - the type or format of the output."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b21211c4-efc6-43dc-9334-ac54faf3b3c7",
      "metadata": {
        "id": "b21211c4-efc6-43dc-9334-ac54faf3b3c7"
      },
      "source": [
        "### Best Practices\n",
        "\n",
        "- Clarity Over Complexity: Clear, simple prompts often yield better results than complex, convoluted ones.\n",
        "- Be Explicit: Clearly state what you want the model to do. Ambiguity can lead to unexpected results.\n",
        "- Context is Key: Provide enough context to guide the model, especially for complex tasks.\n",
        "- Iterate: Continuously refine your prompts based on the outputs you receive. Experimentation is crucial.\n",
        "- Evaluate and Adapt: Regularly evaluate the effectiveness of your prompts and adapt them based on the performance of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8797867f-08d0-430a-8cf1-d5af6f9af6ea",
      "metadata": {
        "id": "8797867f-08d0-430a-8cf1-d5af6f9af6ea"
      },
      "source": [
        "[OpenAI Playground](https://platform.openai.com/playground)"
      ]
    },
    {
      "cell_type": "raw",
      "id": "4960f0ca-dc90-44b9-bce4-3cc9737a4f94",
      "metadata": {
        "id": "4960f0ca-dc90-44b9-bce4-3cc9737a4f94"
      },
      "source": [
        "Prompt:\n",
        "\n",
        "Classify the text into neutral, negative, or positive\n",
        "\n",
        "Text: I think the food was okay.\n",
        "\n",
        "Sentiment:"
      ]
    },
    {
      "cell_type": "raw",
      "id": "99c72df3-dbcb-4269-84cb-887c5a7be06f",
      "metadata": {
        "id": "99c72df3-dbcb-4269-84cb-887c5a7be06f"
      },
      "source": [
        "Prompt:\n",
        "\n",
        "Complete the sentence\n",
        "\n",
        "Sentence: The sky is"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "169d4e57-bfa7-4d5a-acbe-a02146b5ba23",
      "metadata": {
        "id": "169d4e57-bfa7-4d5a-acbe-a02146b5ba23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4b7d18e-5ebe-4130-fcfe-89a7b858160f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.54.5)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.10.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "321974f2-eb81-4bc8-bfea-52c0c669603b",
      "metadata": {
        "id": "321974f2-eb81-4bc8-bfea-52c0c669603b"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import os\n",
        "import openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a412515d-8adb-477d-bf5c-f135e9fd673c",
      "metadata": {
        "id": "a412515d-8adb-477d-bf5c-f135e9fd673c",
        "outputId": "9256f600-e574-4985-ec16-cd15bcee4754",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing config123.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile \"config123.py\"\n",
        "\n",
        "api_key = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b544fcd0-462e-4057-81f3-b0f831f1d113",
      "metadata": {
        "id": "b544fcd0-462e-4057-81f3-b0f831f1d113"
      },
      "outputs": [],
      "source": [
        "openai.api_key = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f7ad866-6603-4793-9be7-303a13e8a3af",
      "metadata": {
        "id": "1f7ad866-6603-4793-9be7-303a13e8a3af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "d8ff28a0-624f-46bb-e603-54f1f57309ca"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'config'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-68f9b6a70845>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# assign api_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"OPENAI_API_KEY\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'config'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from config import api_key\n",
        "# assign api_key\n",
        "openai.api_key = api_key\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26b306c3-b92c-4783-bbe4-c06785424182",
      "metadata": {
        "id": "26b306c3-b92c-4783-bbe4-c06785424182",
        "outputId": "5456b436-bc41-4bc2-e359-cb8bec053621"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The sentiment of the text \"I think the food was okay\" is considered neutral.\n"
          ]
        }
      ],
      "source": [
        "client = openai.OpenAI()\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a AI assistant\"},\n",
        "    {\"role\": \"user\", \"content\": \"\"\"Classify the text into neutral, negative, or positive\n",
        "                        Text: I think the food was okay.\n",
        "                        Sentiment:\"\"\"\n",
        "        }\n",
        "  ]\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ace8dce-2a45-4d7e-9b18-31787fab8284",
      "metadata": {
        "id": "3ace8dce-2a45-4d7e-9b18-31787fab8284",
        "outputId": "637e3a56-42b9-415d-96bf-a221bb1f10f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neutral\n",
            "\n",
            "Reason: The statement expresses a moderate opinion about the food without any strong positive or negative emotions attached. It can be considered a neutral statement.\n"
          ]
        }
      ],
      "source": [
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a AI assistant,Classify the text into neutral, negative, or positive with valid reason of classification\"},\n",
        "    {\"role\": \"user\", \"content\": \"\"\"Text: I think the food was okay.\n",
        "                        Sentiment:\"\"\"\n",
        "        }\n",
        "  ],\n",
        "    temperature = 0.9\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a49d2634-6fd2-41d8-990e-e4ee800f6ffe",
      "metadata": {
        "id": "a49d2634-6fd2-41d8-990e-e4ee800f6ffe",
        "outputId": "fc0c2c60-c3b0-4c0d-fe61-34a344a44a56"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Neutral\\n\\nReason: The statement expresses a moderate opinion about the food without any strong positive or negative emotions attached. It can be considered a neutral statement.'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c89ef70-7ba4-4a1d-8d52-f15cba828dd0",
      "metadata": {
        "id": "1c89ef70-7ba4-4a1d-8d52-f15cba828dd0"
      },
      "source": [
        "#### LLM Settings\n",
        "\n",
        "- Inference parameters in GPT-based models refer to settings and options that can be adjusted to influence the model's behavior and the output it generates during inference (i.e., when the model is used to generate text based on a given prompt).\n",
        "- These parameters help control aspects such as the diversity, coherence, and creativity of the generated text. Here are some common inference parameters:\n",
        "\n",
        "##### Temperature:\n",
        "\n",
        "- Description: Controls the randomness of the model's output. Lower values make the output more focused and deterministic, while higher values make it more diverse and creative.\n",
        "- Range: Typically between 0 and 1.\n",
        "- Example: A temperature of 0.1 results in more predictable and focused responses, whereas a temperature of 0.9 allows for more varied and creative outputs.\n",
        "\n",
        "##### Top-k Sampling:\n",
        "\n",
        "- Description: Limits the next word selection to the top k most likely candidates, reducing the likelihood of choosing less probable words.\n",
        "- Range: Integer value (e.g., 10, 50, 100).\n",
        "- Example: With top-k set to 50, the model considers the 50 most likely next words, adding an element of control while still allowing some diversity.\n",
        "\n",
        "##### Top-p (Nucleus) Sampling:\n",
        "\n",
        "- Description: Limits the next word selection to a subset of words that cumulatively account for a probability mass of p. This method dynamically adjusts the number of candidates considered based on their probabilities.\n",
        "- Range: Typically between 0 and 1.\n",
        "- Example: A top-p value of 0.9 considers the smallest set of words whose probabilities sum to 90%, providing a balance between focus and diversity.\n",
        "\n",
        "##### Max Tokens:\n",
        "\n",
        "- Description: Specifies the maximum number of tokens to generate in the output.\n",
        "- Range: Integer value (e.g., 50, 100, 200).\n",
        "- Example: If max tokens is set to 100, the model will generate up to 100 tokens in the response.\n",
        "\n",
        "##### Presence Penalty:\n",
        "- Description: Discourages the model from repeating words that have already been generated. A higher presence penalty makes the model less likely to repeat tokens.\n",
        "- Range: Typically between 0 and 1.\n",
        "- Example: A presence penalty of 0.5 reduces repetition by penalizing previously used words.\n",
        "\n",
        "##### Frequency Penalty:\n",
        "- Description: Penalizes the model for generating tokens that appear frequently in the text, encouraging more varied output.\n",
        "- Range: Typically between 0 and 1.\n",
        "- Example: A frequency penalty of 0.5 discourages frequent word repetition, promoting diversity in the generated text.\n",
        "##### Stop Sequences:\n",
        "\n",
        "- Description: Specifies one or more sequences of tokens at which the model should stop generating further text.\n",
        "- Range: List of token sequences (e.g., [\"\\n\", \"<|endoftext|>\"]).\n",
        "- Example: If \\n\\n is a stop sequence, the model will stop generating text upon encountering two consecutive newline characters.\n",
        "\n",
        "##### Echo:\n",
        "\n",
        "- Description: If set to true, the model's output includes the input prompt followed by the generated text.\n",
        "- Range: Boolean (true or false).\n",
        "- Example: With echo set to true, the output will contain the prompt text followed by the model's continuation.\n",
        "\n",
        "##### Logprobs:\n",
        "\n",
        "- Description: Returns the log probabilities of the generated tokens, useful for understanding the model's confidence in its choices.\n",
        "- Range: Integer value specifying the number of top log probabilities to return.\n",
        "- Example: If logprobs is set to 5, the model will return log probabilities for the top 5 tokens at each generation step.\n",
        "\n",
        "##### Best Of:\n",
        "- Description: Generates multiple completions server-side and returns the best one based on the lowest log probability per token.\n",
        "- Range: Integer value (e.g., 3, 5, 10).\n",
        "- Example: If best of is set to 5, the model generates 5 completions and returns the best one"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ollama import chat\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "id": "jJPC75dlE8Da",
        "outputId": "98225e17-d718-4b27-b5b3-07b21fa73a4c"
      },
      "id": "jJPC75dlE8Da",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'ollama'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-d6a5270e08d8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mollama\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mchat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ollama'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ad26fd5-f944-4de1-8f83-9bc971dcdff0",
      "metadata": {
        "id": "9ad26fd5-f944-4de1-8f83-9bc971dcdff0"
      },
      "outputs": [],
      "source": [
        "def get_completion(message,temp = 0.7,tokens=500,top_p=1):\n",
        "    completion = client.chat.completions.create(\n",
        "          model=\"gpt-3.5-turbo\",\n",
        "          messages=message,\n",
        "          temperature=temp,\n",
        "          max_tokens=tokens,\n",
        "          top_p=top_p\n",
        "    )\n",
        "    return completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8ce0db3-f5b5-44e1-91e6-29d12fc7f4c8",
      "metadata": {
        "id": "c8ce0db3-f5b5-44e1-91e6-29d12fc7f4c8"
      },
      "source": [
        "Basic prompt example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6100c251-e405-48be-b7d0-05c63a455009",
      "metadata": {
        "id": "6100c251-e405-48be-b7d0-05c63a455009",
        "outputId": "7bdcaa7a-4ac3-4cab-cc28-ca4abf98856a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'client' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-566445a421a7>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     }\n\u001b[1;32m      8\u001b[0m ]\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-c1fffff0c456>\u001b[0m in \u001b[0;36mget_completion\u001b[0;34m(message, temp, tokens, top_p)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtop_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     completion = client.chat.completions.create(\n\u001b[0m\u001b[1;32m      3\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'client' is not defined"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"Predict the next two word in sentence,\n",
        "            Sentence: The sky is\"\"\"\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "response = get_completion(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a522ac5-5b69-4ac0-a952-695f693a205c",
      "metadata": {
        "id": "0a522ac5-5b69-4ac0-a952-695f693a205c",
        "outputId": "2c52366e-1575-4dae-8724-71098b521152",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'get_completion' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d2ab4b5ca1a0>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m ]\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'get_completion' is not defined"
          ]
        }
      ],
      "source": [
        "prompt = \"The sky is\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24735cd9-18c5-4ca0-9d08-b0b7dfe4a7e2",
      "metadata": {
        "id": "24735cd9-18c5-4ca0-9d08-b0b7dfe4a7e2"
      },
      "source": [
        "#### Roles in ChatGPT\n",
        "In the context of ChatGPT and similar large language models, there are typically three main roles: System, User, and Assistant. Each of these roles has a specific purpose in the interaction."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcc242a6-e04a-452f-816f-a1fbba7b452d",
      "metadata": {
        "id": "fcc242a6-e04a-452f-816f-a1fbba7b452d"
      },
      "source": [
        "1. System:\n",
        "\n",
        "- Purpose: The system role provides initial instructions or settings that define the context for the conversation. It sets the behavior and boundaries for the assistant.\n",
        "- The system message is not required but helps to set the overall behavior of the assistant.\n",
        "- Example Use: The system might specify the assistant's capabilities, tone, or other parameters."
      ]
    },
    {
      "cell_type": "raw",
      "id": "c562dfb6-fc41-4d40-b07b-7fb9497d4306",
      "metadata": {
        "id": "c562dfb6-fc41-4d40-b07b-7fb9497d4306"
      },
      "source": [
        "system: You are a helpful assistant that provides concise and accurate information."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f639fcb-8a07-461b-b458-a8a3a59cbb47",
      "metadata": {
        "id": "7f639fcb-8a07-461b-b458-a8a3a59cbb47"
      },
      "source": [
        "2. User\n",
        "- Purpose: The user role represents the person interacting with the assistant. The user asks questions, provides prompts, or seeks assistance on various topics.\n",
        "- Example Use: The user inputs queries or tasks they want the assistant to address."
      ]
    },
    {
      "cell_type": "raw",
      "id": "ea9e3a19-8abf-496c-bd68-ae87a392023f",
      "metadata": {
        "id": "ea9e3a19-8abf-496c-bd68-ae87a392023f"
      },
      "source": [
        "user: What is the capital of France?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60db435f-2594-421e-a995-b4b9c9d35127",
      "metadata": {
        "id": "60db435f-2594-421e-a995-b4b9c9d35127"
      },
      "source": [
        "3. Assistant\n",
        "- Purpose: The assistant role is fulfilled by the AI model. It responds to the user’s queries and follows the instructions provided by the system to deliver helpful and relevant information.\n",
        "- Example Use: The assistant provides answers, explanations, or carries out tasks as requested by the user.\n"
      ]
    },
    {
      "cell_type": "raw",
      "id": "591eef03-800d-4917-9d20-64d23ff0d5b8",
      "metadata": {
        "id": "591eef03-800d-4917-9d20-64d23ff0d5b8"
      },
      "source": [
        "assistant: The capital of France is Paris."
      ]
    },
    {
      "cell_type": "raw",
      "id": "cf9c1521-e8e0-4451-8007-443a3770df7a",
      "metadata": {
        "id": "cf9c1521-e8e0-4451-8007-443a3770df7a"
      },
      "source": [
        "system: You are a knowledgeable assistant who answers questions accurately and concisely.\n",
        "\n",
        "user: Can you help me with my history homework?\n",
        "\n",
        "assistant: Of course! What specific topic or question do you need help with?\n",
        "\n",
        "user: Who was the first President of the United States?\n",
        "\n",
        "assistant: The first President of the United States was George Washington."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99d766dc-15a4-46a0-b720-93a35d1b7cb0",
      "metadata": {
        "id": "99d766dc-15a4-46a0-b720-93a35d1b7cb0",
        "outputId": "4f45d147-ca17-4949-ba9e-5ef1746aee3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Black holes are regions in space where the gravitational pull is so strong that nothing, not even light, can escape from them. They are formed when massive stars collapse under their own gravity at the end of their life cycle. This process is known as supernova explosion. \n",
            "\n",
            "When a massive star exhausts its nuclear fuel, it can no longer support itself against gravity. The core collapses, and if the core's mass is greater than the Chandrasekhar limit of about 1.4 times the mass of the sun, it will continue to collapse into a black hole. The collapse of the core leads to the formation of a singularity, a point of infinite density where the laws of physics as we know them break down.\n",
            "\n",
            "The boundary around a black hole beyond which nothing can escape is called the event horizon. Once an object crosses the event horizon, it is pulled into the black hole and can never escape. Black holes come in different sizes, ranging from stellar-mass black holes, formed from collapsing stars, to supermassive black holes, found at the centers of most galaxies, including our Milky Way.\n"
          ]
        }
      ],
      "source": [
        "messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are an AI research assistant. You use a tone that is technical and scientific.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Hello, who are you?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Greeting! I am an AI research assistant. How can I help you today?\"},\n",
        "        {\"role\": \"user\", \"content\": \"Can you tell me about the creation of black holes?\"}\n",
        "    ]\n",
        "response = get_completion(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0bd6e55-1309-41a5-8836-1210cce1ebe5",
      "metadata": {
        "id": "f0bd6e55-1309-41a5-8836-1210cce1ebe5",
        "outputId": "403ac207-9c74-4832-f8d4-88c9b2916233"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Certainly! Black holes are formed when massive stars reach the end of their life cycle and undergo a supernova explosion. \n",
            "\n",
            "After the explosion, the remaining core collapses under its own gravity, becoming extremely dense and compact. If the core's mass is greater than a certain threshold known as the Chandrasekhar limit, gravity overwhelms all other forces, causing the core to collapse into a singularity - a point of infinite density at the center of a black hole. \n",
            "\n",
            "The region surrounding the singularity, called the event horizon, marks the point of no return where the gravitational pull is so strong that not even light can escape. This is why black holes appear \"black\" and are incredibly difficult to observe directly. \n",
            "\n",
            "Black holes come in different sizes, ranging from stellar-mass black holes formed from the remnants of massive stars to supermassive black holes found at the centers of galaxies, with masses millions or even billions of times that of the Sun.\n"
          ]
        }
      ],
      "source": [
        "messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are an AI research assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Hello, who are you?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Greeting! I am an AI research assistant. How can I help you today?\"},\n",
        "        {\"role\": \"user\", \"content\": \"Can you tell me about the creation of black holes?\"}\n",
        "    ]\n",
        "response = get_completion(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "816b6831-8cba-4ec2-b829-9b252f82702b",
      "metadata": {
        "id": "816b6831-8cba-4ec2-b829-9b252f82702b"
      },
      "outputs": [],
      "source": [
        "message = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an AI assistant\"},\n",
        "        {\"role\": \"user\", \"content\": \"\"\"Classify the text into neutral, negative, or positive\n",
        "                            Text: I think the food was okay.\n",
        "                            Sentiment:\"\"\"\n",
        "            }\n",
        "      ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3d68362-a3ae-468b-b31a-f9c5050cd3fa",
      "metadata": {
        "id": "f3d68362-a3ae-468b-b31a-f9c5050cd3fa",
        "outputId": "769ad909-1fbb-4f17-9f9e-701a04d0928c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The sentiment of the text \"I think the food was okay.\" is neutral.'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_completion(message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd49980f-2d1b-4036-9191-aeb5448ecf0b",
      "metadata": {
        "id": "fd49980f-2d1b-4036-9191-aeb5448ecf0b",
        "outputId": "6661942b-5bd2-4ec4-d980-846c7841d8d4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'🤖💡🔮'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "message = [\n",
        "        {\"role\": \"system\", \"content\": \"You will be provided with text, and your task is to translate it into emojis. Do not use any regular text. Do your best with emojis only.\"},\n",
        "        {\"role\": \"user\", \"content\": \"\"\"Artificial intelligence is a technology with great promise.\"\"\"}\n",
        "      ]\n",
        "get_completion(message)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c01b668f-d302-41cd-bf45-643afeae92fe",
      "metadata": {
        "id": "c01b668f-d302-41cd-bf45-643afeae92fe"
      },
      "source": [
        "### some tips to keep in mind while you are designing your prompts:\n",
        "##### Start simple:\n",
        "- Iterative Process and Experimentation: Designing effective prompts is an iterative process that requires experimentation. Starting with simple prompts and gradually adding more elements and context can help you achieve better results. Utilizing tools like the simple playgrounds provided by OpenAI or Cohere is a great way to begin this experimentation.\n",
        "\n",
        "- Breaking Down Complex Tasks: For large tasks that involve multiple subtasks, it's beneficial to break them down into simpler components. This approach helps avoid overwhelming complexity at the start and allows you to build up the task as you refine your prompts and get better results. Specificity, simplicity, and conciseness are key to achieving optimal outcomes.\n",
        "\n",
        "##### The Instruction\n",
        "- Effective Command Design: You can create effective prompts for various simple tasks by using clear commands such as \"Write\", \"Classify\", \"Summarize\", \"Translate\", and \"Order\". These directives help instruct the model on what you want to achieve, providing a clear goal for the task at hand.\n",
        "\n",
        "- Experimentation and Context: Experimentation is crucial to determine the most effective instructions. Try different keywords, contexts, and data to see what works best for your specific use case. Generally, the more specific and relevant the context is to the task, the better the results. Additionally, placing instructions at the beginning of the prompt and using clear separators like \"###\" to distinguish between instruction and context can enhance clarity and effectiveness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c4abaa5-3a33-408b-9dff-492982e264ce",
      "metadata": {
        "id": "6c4abaa5-3a33-408b-9dff-492982e264ce",
        "outputId": "c91d1bcb-8ac9-45c8-aa2c-86aa1947b29d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\"Hola!\"'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = '''### Instruction ###\n",
        "Translate the text below to Spanish:\n",
        "Text: \"hello!\"'''\n",
        "\n",
        "message = [{\"role\":\"user\",\"content\":prompt}]\n",
        "get_completion(message)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d9ac860-8bda-4f08-8653-96fad177d664",
      "metadata": {
        "id": "0d9ac860-8bda-4f08-8653-96fad177d664"
      },
      "source": [
        "##### Specificity\n",
        "- Descriptive and Detailed Prompts:\n",
        "Be very specific about the instruction and task you want the model to perform. The more descriptive and detailed the prompt is, the better the results. This is particularly important when aiming for a desired outcome or style of generation. While specific tokens or keywords are not the key to success, having a well-formatted and detailed prompt is crucial. Providing examples within the prompt can effectively guide the model to produce the desired output in specific formats.\n",
        "\n",
        "- Relevant and Concise Details:\n",
        "When designing prompts, consider the length and relevance of the details included. There are limitations on prompt length, so it's important to balance specificity with brevity. Avoid including unnecessary details that do not contribute to the task at hand. Instead, focus on relevant details that help achieve the desired outcome. Experimentation and iteration are essential to finding the optimal level of detail and specificity for your prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9692b176-460c-4a74-beac-b46248e5f078",
      "metadata": {
        "id": "9692b176-460c-4a74-beac-b46248e5f078",
        "outputId": "50962ffa-9124-42e1-9cf8-4c9775be56ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Place: Champalimaud Centre for the Unknown, Lisbon'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "message = [\n",
        "    {\"role\":\"user\",\"content\":'''Extract the name of places in the following text.\n",
        "                Desired format:\n",
        "                Place: <comma_separated_list_of_places>\n",
        "                Input: \"Although these developments are encouraging to researchers, much is still a mystery.\n",
        "                “We often have a black box between the brain and the effect we see in the periphery,”\n",
        "                says Henrique Veiga-Fernandes, a neuroimmunologist at the Champalimaud Centre for the Unknown in Lisbon. .\n",
        "                \"If we want to use it in the therapeutic context, we actually need to understand the mechanism.\"'''\n",
        "    }\n",
        "]\n",
        "get_completion(message)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e466e6cf-4388-44ee-aae0-e9869642f79c",
      "metadata": {
        "id": "e466e6cf-4388-44ee-aae0-e9869642f79c"
      },
      "source": [
        "##### Utilizing Separator Tokens/ use delimiters\n",
        "- Separator tokens are special characters or strings used to clearly distinguish between different parts of a prompt or text input. They help structure the input more clearly and guide the model to understand the distinct sections within a prompt.\n",
        "- Separator tokens can improve the model's comprehension and the quality of its responses, especially when dealing with complex prompts that include multiple components such as instructions, context, and examples.\n",
        "- Use delimiters in your prompt to separate the prompt instructions (system instructions) from the user input. You can use any punctuation symbol that tells the model the separation between the instructions and input.\n",
        "\n",
        "Common Separator Tokens\n",
        "\n",
        "- Triple quotes “““\n",
        "- Triple back-ticks ```\n",
        "- Triple dashes ---\n",
        "- Angle brackets < >\n",
        "- XML tags <tag></tag>\n",
        "- \"###\"\n",
        "- \"---\"\n",
        "\n",
        "Importance of Separator Tokens\n",
        "- Clarity: Makes the structure of the prompt more explicit, reducing ambiguity.\n",
        "- Segmentation: Helps in clearly delineating different sections like instructions, examples, and the main task.\n",
        "- Readability: Improves readability for both the model and human users, making it easier to debug and refine prompts."
      ]
    },
    {
      "cell_type": "raw",
      "id": "b7911656-6044-406d-aaba-4495b2062ad8",
      "metadata": {
        "id": "b7911656-6044-406d-aaba-4495b2062ad8"
      },
      "source": [
        "### Instruction:\n",
        "Summarize the following text:\n",
        "\n",
        "### Text:\n",
        "\"The quick brown fox jumps over the lazy dog.\"\n"
      ]
    },
    {
      "cell_type": "raw",
      "id": "b76add90-26bf-4f2f-982b-2521e3c3b936",
      "metadata": {
        "id": "b76add90-26bf-4f2f-982b-2521e3c3b936"
      },
      "source": [
        "### Instruction:\n",
        "Translate the following English sentences to French:\n",
        "\n",
        "### Examples:\n",
        "1. English: Hello, how are you?\n",
        "   French: Bonjour, comment ça va?\n",
        "2. English: Good morning.\n",
        "   French: Bonjour.\n",
        "\n",
        "### Task:\n",
        "3. English: I love programming.\n",
        "   French:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36bbbb29-3812-4008-8092-b8077e185573",
      "metadata": {
        "id": "36bbbb29-3812-4008-8092-b8077e185573",
        "outputId": "47119510-33e3-4536-bfb7-bd51e479dd0d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"J'adore la programmation.\""
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = '''### Instruction:\n",
        "Translate the following English sentences to French:\n",
        "\n",
        "### Examples:\n",
        "1. English: Hello, how are you?\n",
        "   French: Bonjour, comment ça va?\n",
        "2. English: Good morning.\n",
        "   French: Bonjour.\n",
        "\n",
        "### Task:\n",
        "3. English: I love programming.\n",
        "   French:'''\n",
        "\n",
        "message = [{\"role\":\"user\",\"content\":prompt}]\n",
        "get_completion(message)"
      ]
    },
    {
      "cell_type": "raw",
      "id": "03552487-fd7a-41aa-9886-bb53dc6ee75a",
      "metadata": {
        "id": "03552487-fd7a-41aa-9886-bb53dc6ee75a"
      },
      "source": [
        "reviews = \"\"\"This is an over engineered waste of money\n",
        "\n",
        "Nobody is truly going to be wearing this unless your in the frozen tundra…. Zero mobility way to large in length… Absolute waste…\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "You will be provided with a list of product reviews delimited by <>.\n",
        "Analyze each reviews and provide an overview of customers satisfaction, listing\n",
        "positive and negative aspects reported by customers.\n",
        "\n",
        "<{reviews}>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cedb1e3-278c-4218-b492-caaf40b4336f",
      "metadata": {
        "id": "6cedb1e3-278c-4218-b492-caaf40b4336f",
        "outputId": "eca8cf30-f2d6-4544-f2e2-412988b8b8c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Customer Satisfaction Overview:\n",
            "Negative aspects reported:\n",
            "1. Over engineered\n",
            "2. Waste of money\n",
            "3. Not practical for everyday wear\n",
            "4. Zero mobility\n",
            "5. Too large in length\n",
            "\n",
            "Overall, customers seem to be dissatisfied with the product, citing various reasons such as impracticality, lack of mobility, and being over engineered. The general consensus is that it is a waste of money.\n"
          ]
        }
      ],
      "source": [
        "reviews = \"\"\"This is an over engineered waste of money\n",
        "\n",
        "Nobody is truly going to be wearing this unless your in the frozen tundra…. Zero mobility way to large in length… Absolute waste…\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "You will be provided with a list of product reviews delimited by <>.\n",
        "Analyze each reviews and provide an overview of customers satisfaction, listing\n",
        "positive and negative aspects reported by customers.\n",
        "\n",
        "<{reviews}>\n",
        "\"\"\"\n",
        "\n",
        "message = [{\"role\":\"user\",\"content\":prompt}]\n",
        "response = get_completion(message)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6a63cdb-a014-42de-aa79-7a376a22a130",
      "metadata": {
        "id": "b6a63cdb-a014-42de-aa79-7a376a22a130",
        "outputId": "4f01a92a-6ad7-455e-d3c5-22de5f70bf82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"positive_aspects\": [],\n",
            "  \"negative_aspects\": [\n",
            "    \"Over engineered\",\n",
            "    \"Waste of money\",\n",
            "    \"Zero mobility\",\n",
            "    \"Too large in length\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "reviews = \"\"\"This is an over engineered waste of money\n",
        "\n",
        "Nobody is truly going to be wearing this unless your in the frozen tundra…. Zero mobility way to large in length… Absolute waste…\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "You will be provided with a list of product reviews delimited by <>.\n",
        "Analyze each reviews and provide an overview of customers satisfaction, listing\n",
        "positive and negative aspects reported by customers.\n",
        "\n",
        "Provide them in JSON format with the following keys:\n",
        "positive_aspects, negative_aspects\n",
        "\n",
        "<{reviews}>\n",
        "\"\"\"\n",
        "\n",
        "message = [{\"role\":\"user\",\"content\":prompt}]\n",
        "response = get_completion(message)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbe136e8-9ae8-4a98-b640-634eb2417799",
      "metadata": {
        "id": "cbe136e8-9ae8-4a98-b640-634eb2417799"
      },
      "source": [
        "##### Avoid Impreciseness\n",
        "- Given the tips above about being detailed and improving format, it's easy to fall into the trap of wanting to be too clever about prompts and potentially creating imprecise descriptions. It's often better to be specific and direct. The analogy here is very similar to effective communication -- the more direct, the more effective the message gets across."
      ]
    },
    {
      "cell_type": "raw",
      "id": "b1a6c754-35cf-4076-994d-327420e6a6f8",
      "metadata": {
        "id": "b1a6c754-35cf-4076-994d-327420e6a6f8"
      },
      "source": [
        "prompt:\n",
        "Explain the concept prompt engineering. Keep the explanation short, only a few sentences, and don't be too descriptive.\n",
        "\n",
        "Better prompt:\n",
        "Use 2-3 sentences to explain the concept of prompt engineering to a high school student."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6236f6fc-2805-49fe-8fb9-7c56051adb5e",
      "metadata": {
        "id": "6236f6fc-2805-49fe-8fb9-7c56051adb5e"
      },
      "source": [
        "To do or not to do?\n",
        "- Another common tip when designing prompts is to avoid saying what not to do but say what to do instead. This encourages more specificity and focuses on the details that lead to good responses from the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a71e8f8-05f6-4b95-b502-b871830e823a",
      "metadata": {
        "id": "3a71e8f8-05f6-4b95-b502-b871830e823a",
        "outputId": "15dd6ef5-0de5-42d0-e221-4b137cb57d8d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'I\\'d be happy to recommend a movie for you! How about checking out the movie \"Inception\"? It\\'s a mind-bending thriller that will keep you on the edge of your seat. Let me know if you\\'d like more recommendations!'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = \"\"\"The following is an agent that recommends movies to a customer. DO NOT ASK FOR INTERESTS. DO NOT ASK FOR PERSONAL INFORMATION.\n",
        "\n",
        "Customer: Please recommend a movie based on my interests.\n",
        "Agent: \"\"\"\n",
        "message = [{\"role\":\"user\",\"content\":prompt}]\n",
        "get_completion(message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f6b2dc3-3d7e-42b3-94ce-7b69d9ffc10c",
      "metadata": {
        "id": "1f6b2dc3-3d7e-42b3-94ce-7b69d9ffc10c",
        "outputId": "0711bb39-ecbb-4dcb-e5b0-62cd3b48b3d3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"I'm here to recommend a movie from the top global trending movies. Let me find one for you.\""
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = \"\"\"The following is an agent that recommends movies to a customer. The agent is responsible to recommend a movie from the top global trending movies. It should refrain from asking users for their preferences and avoid asking for personal information. If the agent doesn't have a movie to recommend, it should respond \"Sorry, couldn't find a movie to recommend today.\".\n",
        "Customer: Please recommend a movie based on my interests.\n",
        "Agent: \"\"\"\n",
        "message = [{\"role\":\"user\",\"content\":prompt}]\n",
        "get_completion(message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f05bb872-ae29-4375-8c2d-bf2fa9257799",
      "metadata": {
        "id": "f05bb872-ae29-4375-8c2d-bf2fa9257799"
      },
      "outputs": [],
      "source": [
        "def get_completion(message,temp = 0.7,tokens=500,top_p=1):\n",
        "    completion = client.chat.completions.create(\n",
        "          model=\"gpt-4o-mini\",\n",
        "          messages=message,\n",
        "          temperature=temp,\n",
        "          max_tokens=tokens,\n",
        "          top_p=top_p\n",
        "    )\n",
        "    return completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ceaab859-4c22-4bcb-89f9-a7a0ccdfee24",
      "metadata": {
        "id": "ceaab859-4c22-4bcb-89f9-a7a0ccdfee24",
        "outputId": "e2808c8c-6a56-487e-8b35-4c026160b7a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "As of my last update in October 2023, the 2024 ICC Men's T20 World Cup had not yet taken place, so I do not have information on the winner. For the latest updates, please check current sports news sources or the official ICC website.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"who won the cricket t2020 world cup in 2024?\"\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "992a700f-8393-4e27-9758-4ac0739efada",
      "metadata": {
        "id": "992a700f-8393-4e27-9758-4ac0739efada"
      },
      "source": [
        "#### Text Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb515802-8742-4a1f-8324-d6d74c351abe",
      "metadata": {
        "id": "fb515802-8742-4a1f-8324-d6d74c351abe",
        "outputId": "1b72d7a7-5953-49fd-cac7-b625470c9cb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Antibiotics are medications used to treat bacterial infections by killing or inhibiting the growth of bacteria, but they are ineffective against viral infections and misuse can lead to antibiotic resistance.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"Antibiotics are a type of medication used to treat bacterial infections. They work by either killing the bacteria or preventing them from reproducing, allowing the body's immune system to fight off the infection. Antibiotics are usually taken orally in the form of pills, capsules, or liquid solutions, or sometimes administered intravenously. They are not effective against viral infections, and using them inappropriately can lead to antibiotic resistance.\n",
        "\n",
        "Explain the above in one sentence:\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77b195c1-04e2-42e6-9d55-d193a862f43a",
      "metadata": {
        "id": "77b195c1-04e2-42e6-9d55-d193a862f43a",
        "outputId": "b9f3995a-4ccb-49de-c435-6434ac18c287"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Jupiter is a big planet in our Solar System that is fifth from the Sun. It is very large and mostly made of gas. Even though it is much smaller than the Sun, it is still bigger than all the other planets combined. Jupiter is very bright and can be seen without a telescope. It has been known to people for a long time and is named after a Roman god. When we look at Jupiter from Earth, it can be so bright that it casts shadows at night.\n"
          ]
        }
      ],
      "source": [
        "system = \"\"\"Summarize content you are provided with for a second-grade student. \"\"\"\n",
        "user = \"\"\"Jupiter is the fifth planet from the Sun and the largest in the Solar System.\n",
        "    It is a gas giant with a mass one-thousandth that of the Sun, but two-and-a-half times that of all the other planets in the Solar System combined.\n",
        "    Jupiter is one of the brightest objects visible to the naked eye in the night sky, and has been known to ancient civilizations since before recorded history.\n",
        "    It is named after the Roman god Jupiter.[19] When viewed from Earth, Jupiter can be bright enough for its reflected light to cast visible shadows,[20] and is on average the third-brightest natural object in the night sky after the Moon and Venus.\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": system\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": user\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67652329-87cb-44aa-b795-5e4956895929",
      "metadata": {
        "id": "67652329-87cb-44aa-b795-5e4956895929",
        "outputId": "13e210e9-6cf3-4e42-eed4-f013790d8487"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Jupiter is the largest planet in the Solar System and is a gas giant. It is the fifth planet from the Sun and has a mass two-and-a-half times that of all other planets combined. Jupiter is visible to the naked eye and has been known since ancient times. It is named after the Roman god Jupiter.\n"
          ]
        }
      ],
      "source": [
        "system = \"\"\"Summarize content you are provided in 50 words \"\"\"\n",
        "user = \"\"\"Jupiter is the fifth planet from the Sun and the largest in the Solar System.\n",
        "    It is a gas giant with a mass one-thousandth that of the Sun, but two-and-a-half times that of all the other planets in the Solar System combined.\n",
        "    Jupiter is one of the brightest objects visible to the naked eye in the night sky, and has been known to ancient civilizations since before recorded history.\n",
        "    It is named after the Roman god Jupiter.[19] When viewed from Earth, Jupiter can be bright enough for its reflected light to cast visible shadows,[20] and is on average the third-brightest natural object in the night sky after the Moon and Venus.\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": system\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": user\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a06b9e3e-ddb5-492d-a7d4-fd9cbedf0fae",
      "metadata": {
        "id": "a06b9e3e-ddb5-492d-a7d4-fd9cbedf0fae"
      },
      "source": [
        "#### Information Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95291504-c2e5-48f4-9119-0a42f373530d",
      "metadata": {
        "id": "95291504-c2e5-48f4-9119-0a42f373530d",
        "outputId": "bf24addb-fec4-4b2a-e8f5-2f9f933dac25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatGPT\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"Author-contribution statements and acknowledgements in research papers should state clearly and specifically whether, and to what extent, the authors used AI technologies such as ChatGPT in the preparation of their manuscript and analysis.\n",
        "They should also indicate which LLMs were used. This will alert editors and reviewers to scrutinize manuscripts more carefully for potential biases, inaccuracies, and improper source crediting. Likewise, scientific journals should be transparent about their use of LLMs, for example when selecting submitted manuscripts.\n",
        "\n",
        "Identify the large language model-based product mentioned in the paragraph above:\n",
        "\"\"\"\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69e5804c-6fe6-46ef-add0-f9eea8c2681f",
      "metadata": {
        "id": "69e5804c-6fe6-46ef-add0-f9eea8c2681f",
        "outputId": "7b218d29-512b-4e08-99c0-96ebe860f1d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"people\": [\n",
            "    \"Angela Merkel\"\n",
            "  ],\n",
            "  \"organizations\": [\n",
            "    \"European Union\",\n",
            "    \"Time\"\n",
            "  ],\n",
            "  \"locations\": [\n",
            "    \"Germany\",\n",
            "    \"Hamburg\",\n",
            "    \"University of Leipzig\",\n",
            "    \"Central Institute for Physical Chemistry\",\n",
            "    \"East Berlin\"\n",
            "  ],\n",
            "  \"dates\": [\n",
            "    \"July 17, 1954\",\n",
            "    \"2015\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "prompt = '''Extract and list all named entities such as people, organizations, locations, dates, and other relevant entities from the following text. Provide the entities in a structured JSON format:\n",
        "\n",
        "Text:\n",
        "\"Angela Merkel, the Chancellor of Germany, was born in Hamburg on July 17, 1954. She has been a leading figure in the European Union and won the Time Person of the Year award in 2015. Merkel studied physics at the University of Leipzig and earned her doctorate at the Central Institute for Physical Chemistry in East Berlin.\"\n",
        "\n",
        "Entities:\n",
        "'''\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfdad336-c0cc-4af8-91c3-a93fabbc8e9e",
      "metadata": {
        "id": "cfdad336-c0cc-4af8-91c3-a93fabbc8e9e"
      },
      "source": [
        "#### Question Answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "894a0024-a049-429d-a15b-5ae64168bfa9",
      "metadata": {
        "id": "894a0024-a049-429d-a15b-5ae64168bfa9",
        "outputId": "f999cef9-360f-4366-ede0-ab151aedfa2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mice\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"Answer the question based on the context below. Keep the answer short and concise. Respond \"Unsure about answer\" if not sure about the answer.\n",
        "\n",
        "Context: Teplizumab traces its roots to a New Jersey drug company called Ortho Pharmaceutical. There, scientists generated an early version of the antibody, dubbed OKT3. Originally sourced from mice, the molecule was able to bind to the surface of T cells and limit their cell-killing potential. In 1986, it was approved to help prevent organ rejection after kidney transplants, making it the first therapeutic antibody allowed for human use.\n",
        "\n",
        "Question: What was OKT3 originally sourced from?\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce2cca9a-c8e5-4dca-9f85-21e14d9251fc",
      "metadata": {
        "id": "ce2cca9a-c8e5-4dca-9f85-21e14d9251fc"
      },
      "source": [
        "#### Text Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55769169-c944-400c-85e1-d6a22a9ae4fc",
      "metadata": {
        "id": "55769169-c944-400c-85e1-d6a22a9ae4fc",
        "outputId": "b7348923-8d07-4f0a-c207-4cc3e3b62aa9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neutral\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"Classify the text into neutral, negative or positive.\n",
        "\n",
        "Text: I think the food was okay.\n",
        "\n",
        "Sentiment:\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f793e7b-daf9-44de-b0e0-d3d7a7c741fd",
      "metadata": {
        "id": "2f793e7b-daf9-44de-b0e0-d3d7a7c741fd",
        "outputId": "ae4aea5f-c7cd-4487-b2a8-ffd04ca9d8ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neutral\n"
          ]
        }
      ],
      "source": [
        "system = \"You are a sentiment analyst. Classify the text as neutral, negative, or positive.\"\n",
        "prompt = \"I think the food was okay.\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\":\"system\",\n",
        "        \"content\":system\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20d544a8-fb11-46dc-ae8c-b0cd5102dc19",
      "metadata": {
        "id": "20d544a8-fb11-46dc-ae8c-b0cd5102dc19",
        "outputId": "c1567890-54ca-429c-f585-8102724a4f25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " neutral\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"Classify the text into neutral, negative or positive.\n",
        "\n",
        "Text: I think the vacation is okay.\n",
        "Sentiment: neutral\n",
        "\n",
        "Text: I think the food was okay.\n",
        "Sentiment:\"\"\"\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de22f7ce-20c4-42f3-829a-24599a25ec3b",
      "metadata": {
        "id": "de22f7ce-20c4-42f3-829a-24599a25ec3b",
        "outputId": "e7500ad4-32a0-46b1-d684-9efee7c16926"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MS Dhoni Casts Vote in Lok Sabha Polls, Election Commission Shares 'Thala for a Reason' Picture\n"
          ]
        }
      ],
      "source": [
        "prompt = '''Former India captain and Chennai Super Kings player MS Dhoni cast his vote for Phase 6 of the Lok Sabha polls at a polling station in Ranchi. The Election Commission of India shared the picture of MS Dhoni, along with his family, casting his vote for the Lok Sabha polls in Ranchi, while also capturing the famous social media meme 'thala for a reason'.\n",
        "\n",
        "Lauding Dhoni for following his responsibility as a citizen, Election Commission wrote on X (formerly Twitter), “Thala for a reason !! Dhoni hits a six for democracy by casting his vote along with his family”\n",
        "\n",
        "The CSK batsman was last seen in action during the IPL 2024 clash between Royal Challengers Bengaluru and Chennai Super Kings, in which his team lost by 27 runs and failed to secure a place in the IPL play-offs. Notably, Dhoni skipped the customary handshake with the RCB players after the match and has since been expected to announce a decision on his retirement soon.\n",
        "MS Dhoni prefers Instagram over Twitter:\n",
        "The former Indian captain had recently talked about why prefers to use Instagram over X. Dhoni stated that he likes Instagram because it allows him to share photos or videos and then disengage.\n",
        "\n",
        "In a video posted by Dubai Eye 103.8 on YouTube, Dhoni also explains why he isn't impressed with X, stating \"\"I prefer Instagram over Twitter. I believe that nothing good has happened over Twitter, especially in India. There is always a controversy. Somebody will write something, and it turns into a controversy,\"\n",
        "\n",
        "“It used to be 140 characters. You cannot elaborate. Imagine me putting something over there, and then it is left up to the people to read it. And, they just interpret what they want to interpret,\" Dhoni added'''\n",
        "\n",
        "system = \"You are a news assistant. return the news headline for given description\"\n",
        "# prompt = \"I think the food was okay.\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\":\"system\",\n",
        "        \"content\":system\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "294a75d0-2dc6-44b8-8430-5eb1fd8b41ae",
      "metadata": {
        "id": "294a75d0-2dc6-44b8-8430-5eb1fd8b41ae"
      },
      "source": [
        "MS Dhoni casts his vote in Ranchi during Lok Sabha 2024 Polls Phase 6; Election Commission says ‘thala for a reason’"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "814126a6-3ea7-483a-bfce-98832234c50e",
      "metadata": {
        "id": "814126a6-3ea7-483a-bfce-98832234c50e"
      },
      "source": [
        "#### Conversation\n",
        "One of the more interesting things you can achieve with prompt engineering is instructing the LLM system on how to behave, its intent, and its identity. This is particularly useful when you are building conversational systems like customer service chatbots. This is sometimes referred to as role prompting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f95f0559-85e7-4e1c-870d-921afbd14db9",
      "metadata": {
        "id": "f95f0559-85e7-4e1c-870d-921afbd14db9",
        "outputId": "badf155d-5c90-4dc8-dda6-210e1fce7ca4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Certainly! Black holes are formed from the gravitational collapse of massive objects, typically the remnants of massive stars. The process can be outlined in several key stages:\n",
            "\n",
            "1. **Stellar Evolution**: A star spends most of its life in a stable phase known as the main sequence, where it fuses hydrogen into helium in its core. As the star exhausts its hydrogen fuel, it undergoes a series of nuclear fusion processes that lead to the formation of heavier elements.\n",
            "\n",
            "2. **Supernova Explosion**: For stars that are significantly more massive than the Sun (typically more than about 20 solar masses), once they exhaust their nuclear fuel, they can no longer support themselves against gravitational collapse. This leads to an implosion of the core, followed by a violent explosion known as a supernova. The outer layers are expelled into space, while the core collapses.\n",
            "\n",
            "3. **Formation of a Black Hole**: If the remaining core has a mass greater than the Tolman-Oppenheimer-Volkoff limit (approximately 2 to 3 solar masses), it collapses under its own gravity to a point of infinite density known as a singularity. The region surrounding this singularity where the escape velocity exceeds the speed of light is termed the event horizon, marking the boundary of the black hole.\n",
            "\n",
            "4. **Types of Black Holes**: There are several types of black holes:\n",
            "   - **Stellar Black Holes**: Formed from the remnants of massive stars.\n",
            "   - **Supermassive Black Holes**: Found at the centers of galaxies, containing millions to billions of solar masses, their formation mechanisms are still under investigation, but they may form through the merging of smaller black holes and accretion of mass over time.\n",
            "   - **Intermediate Black Holes**: These are hypothesized to exist in the mass range between stellar and supermassive black holes, but their existence is less certain.\n",
            "   - **Primordial Black Holes**: Hypothetical black holes that could have formed in the early universe due to density fluctuations.\n",
            "\n",
            "5. **Detection**: Black holes cannot be observed directly due to their nature; however, their presence can be inferred through their gravitational effects on nearby stars and gas. The emission of X-rays from accreting material and gravitational waves from merging black holes are also key methods of detection.\n",
            "\n",
            "The study of black holes is a significant area of research in astrophysics, with implications for our understanding of gravity, quantum mechanics,\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"The following is a conversation with an AI research assistant. The assistant tone is technical and scientific.\n",
        "\n",
        "Human: Hello, who are you?\n",
        "AI: Greeting! I am an AI research assistant. How can I help you today?\n",
        "Human: Can you tell me about the creation of blackholes?\n",
        "AI:\"\"\"\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94fcaa45-4100-48f9-b93a-ba683d52a496",
      "metadata": {
        "id": "94fcaa45-4100-48f9-b93a-ba683d52a496",
        "outputId": "d55b49a0-83a9-4c8a-879f-66b7fc406b13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sure! Black holes are created when a massive star runs out of fuel and collapses under its own gravity. This collapse causes the star's core to become extremely dense, forming a region with a gravitational pull so strong that not even light can escape. This is what we call a black hole.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"The following is a conversation with an AI research assistant. The assistant answers should be easy to understand even by primary school students.\n",
        "\n",
        "Human: Hello, who are you?\n",
        "AI: Greeting! I am an AI research assistant. How can I help you today?\n",
        "Human: Can you tell me about the creation of black holes?\n",
        "AI: \"\"\"\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "424be704-a120-4e5a-a10e-4ac639828867",
      "metadata": {
        "id": "424be704-a120-4e5a-a10e-4ac639828867",
        "outputId": "121138a4-30e3-4246-8d52-482580a65be7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The creation of nuclear bombs involves the process of nuclear fission, where the nucleus of an atom is split into two or more smaller nuclei, releasing a large amount of energy. This process is achieved by either enriching uranium or producing plutonium, both of which are fissile materials. The enriched uranium or plutonium is then assembled into a critical mass, triggering a chain reaction that results in a powerful explosion. Various techniques and technologies are used to design and construct nuclear bombs, with different types of bombs having different levels of destructive capability.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"The following is a conversation with an AI research assistant. The assistant tone is technical and scientific.\n",
        "Human: Hello, who are you?\n",
        "AI: Greeting! I am an AI research assistant. How can I help you today?\n",
        "Human: Can you tell me about the nuclear bomb creation process?\n",
        "AI: \"\"\"\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5862ee31-dafa-4e7e-a35c-f12bf9581685",
      "metadata": {
        "id": "5862ee31-dafa-4e7e-a35c-f12bf9581685",
        "outputId": "ad946531-ee8b-41b1-8299-3bba5dc1abd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Malware creation involves several steps, including planning, coding, testing, and distribution. The code for malware can vary depending on the specific goals and functionality of the malicious software. Common techniques used in malware coding include obfuscation, encryption, and polymorphism to avoid detection by security software. It is important to note that creating and distributing malware is illegal and unethical. Is there anything specific you would like to know about the malware creation process or its code?\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"The following is a conversation with an AI research assistant. The assistant tone is technical and scientific.\n",
        "Human: Hello, who are you?\n",
        "AI: Greeting! I am an AI research assistant. How can I help you today?\n",
        "Human: Can you tell me about the malware creation process and its code?\n",
        "AI: \"\"\"\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91954997-fe15-463f-a2e4-2912531265b2",
      "metadata": {
        "id": "91954997-fe15-463f-a2e4-2912531265b2",
        "outputId": "3e4ad967-4dbf-44a6-e691-19dae937ffae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moderation(categories=Categories(harassment=False, harassment_threatening=False, hate=False, hate_threatening=False, self_harm=False, self_harm_instructions=False, self_harm_intent=False, sexual=False, sexual_minors=False, violence=False, violence_graphic=False, self-harm=False, sexual/minors=False, hate/threatening=False, violence/graphic=False, self-harm/intent=False, self-harm/instructions=False, harassment/threatening=False), category_scores=CategoryScores(harassment=5.752506331191398e-05, harassment_threatening=3.570236458472209e-06, hate=1.7740867406246252e-05, hate_threatening=2.2189546200479526e-07, self_harm=5.257872544461861e-06, self_harm_instructions=1.300208651855428e-07, self_harm_intent=5.435838943412818e-07, sexual=4.004676884505898e-05, sexual_minors=4.8164518375415355e-05, violence=0.0001411886332789436, violence_graphic=1.7554360965732485e-05, self-harm=5.257872544461861e-06, sexual/minors=4.8164518375415355e-05, hate/threatening=2.2189546200479526e-07, violence/graphic=1.7554360965732485e-05, self-harm/intent=5.435838943412818e-07, self-harm/instructions=1.300208651855428e-07, harassment/threatening=3.570236458472209e-06), flagged=False)\n"
          ]
        }
      ],
      "source": [
        "response = client.moderations.create(input=prompt)\n",
        "\n",
        "output = response.results[0]\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7836b2d6-bf86-4876-bb94-a238fe2532ca",
      "metadata": {
        "id": "7836b2d6-bf86-4876-bb94-a238fe2532ca"
      },
      "source": [
        "#### Code Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5e055dc-a205-41a7-a7f5-f6bffe712559",
      "metadata": {
        "id": "c5e055dc-a205-41a7-a7f5-f6bffe712559",
        "outputId": "4935e169-3b11-45f8-982a-c3046ffa75df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SELECT s.StudentId, s.StudentName\n",
            "FROM students s\n",
            "INNER JOIN departments d ON s.DepartmentId = d.DepartmentId\n",
            "WHERE d.DepartmentName = 'Computer Science'\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\\\"\\\"\\\"\\nTable departments, columns = [DepartmentId, DepartmentName]\\nTable students, columns = [DepartmentId, StudentId, StudentName]\\nCreate a MySQL query for all students in the Computer Science Department\\n\\\"\\\"\\\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "824142ec-54c4-4c54-8de6-4f93ddfcc347",
      "metadata": {
        "id": "824142ec-54c4-4c54-8de6-4f93ddfcc347",
        "outputId": "8ed99822-e1fa-4fd4-e8f5-9ec108bc47b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SELECT StudentId, StudentName\n",
            "FROM students\n",
            "WHERE DepartmentId = (SELECT DepartmentId FROM departments WHERE DepartmentName = 'Computer Science')\n"
          ]
        }
      ],
      "source": [
        "system = \"You are a SQL query expert, return only query, avoid extra words\"\n",
        "user = \"\"\"\n",
        "Table departments, columns = [DepartmentId, DepartmentName]\n",
        "Table students, columns = [DepartmentId, StudentId, StudentName]\n",
        "Create a MySQL query for all students in the Computer Science Department.\n",
        "\"\"\"\n",
        "messages = [\n",
        "    {\n",
        "        \"role\":\"system\",\n",
        "        \"content\":system\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91f5a6c0-505a-4caa-a4fd-6e3715050773",
      "metadata": {
        "id": "91f5a6c0-505a-4caa-a4fd-6e3715050773",
        "outputId": "0a1bffe0-1b26-4cd9-ca1d-009a685b5dda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SELECT StudentId, StudentName\n",
            "FROM students\n",
            "WHERE DepartmentId = (SELECT DepartmentId FROM departments WHERE DepartmentName = 'Computer Science');\n"
          ]
        }
      ],
      "source": [
        "system = \"You are a SQL query expert. Write only the SQL query in output\"\n",
        "user = \"\"\"\n",
        "Table departments, columns = [DepartmentId, DepartmentName]\n",
        "Table students, columns = [DepartmentId, StudentId, StudentName]\n",
        "Create a MySQL query for all students in the Computer Science Department.\n",
        "\"\"\"\n",
        "messages = [\n",
        "    {\n",
        "        \"role\":\"system\",\n",
        "        \"content\":system\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6b2022a-bb34-430e-bb62-c6a200d6aa09",
      "metadata": {
        "id": "d6b2022a-bb34-430e-bb62-c6a200d6aa09"
      },
      "source": [
        "#### Reasoning\n",
        "\n",
        "The most difficult tasks for an LLM today is one that requires some form of reasoning. Reasoning is one of most interesting areas due to the types of complex applications that can emerge from LLMs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6503dfda-a2f7-48ad-afd2-20ef24b1fc03",
      "metadata": {
        "id": "6503dfda-a2f7-48ad-afd2-20ef24b1fc03",
        "outputId": "f0dd9203-b7f3-4695-a21c-1f4f50a91c14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The odd numbers in this group are 15, 5, 13, 7, and 1. \n",
            "When you add them up, you get 15 + 5 + 13 + 7 + 1 = 41, which is an odd number.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
        "A: \"\"\"\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion( messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f024bd8-8dbd-45b5-a28a-90d762a09053",
      "metadata": {
        "id": "9f024bd8-8dbd-45b5-a28a-90d762a09053",
        "outputId": "3b24cbff-be95-49f2-9a4b-1de52341bccb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Odd numbers in the group: 15, 5, 13, 7, 1\n",
            "\n",
            "Adding the odd numbers: 15 + 5 + 13 + 7 + 1 = 41\n",
            "\n",
            "41 is an odd number.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
        "\n",
        "Solve by breaking the problem into steps. First, identify the odd numbers, add them, and indicate whether the result is odd or even.\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion( messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91b95dfd-ad26-4d8b-b9e3-1797f4c8b0cd",
      "metadata": {
        "id": "91b95dfd-ad26-4d8b-b9e3-1797f4c8b0cd"
      },
      "source": [
        "<hr style=\"border: 2px solid black; margin: 5px 0;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53b7979d-ddc7-4508-aaba-23e26a47c19c",
      "metadata": {
        "id": "53b7979d-ddc7-4508-aaba-23e26a47c19c"
      },
      "source": [
        "### Prompting Techniques\n",
        "Prompt Engineering helps to effectively design and improve prompts to get better results on different tasks with LLMs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2e93d24-ad68-455f-9a6e-638a58658afc",
      "metadata": {
        "id": "c2e93d24-ad68-455f-9a6e-638a58658afc"
      },
      "source": [
        "#### Zero-Shot Prompting:\n",
        "\n",
        "- Modern LLMs like GPT-3.5 Turbo, GPT-4, and Claude 3 are designed to follow instructions effectively.\n",
        "These models are trained on vast amounts of data, enabling them to perform tasks in a \"zero-shot\" manner.\n",
        "- Zero-shot prompting involves using a prompt that does not include examples or demonstrations.\n",
        "The prompt directly instructs the model to perform a task without providing additional examples to guide it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dacc865-3e17-4f18-abc9-751101eb7ac6",
      "metadata": {
        "id": "6dacc865-3e17-4f18-abc9-751101eb7ac6",
        "outputId": "140fb01a-e827-4f66-93eb-a4e52257c83a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neutral\n"
          ]
        }
      ],
      "source": [
        "prompt = '''Classify the text into neutral, negative or positive.\n",
        "Text: I think the vacation is okay.\n",
        "Sentiment:'''\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion( messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5e1c354-b542-472c-9ad7-0f0b794be364",
      "metadata": {
        "id": "f5e1c354-b542-472c-9ad7-0f0b794be364"
      },
      "source": [
        "<hr style=\"border: 2px solid black; margin: 5px 0;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fda03d95-e1a7-448b-8ce2-af0d67a3d185",
      "metadata": {
        "id": "fda03d95-e1a7-448b-8ce2-af0d67a3d185"
      },
      "source": [
        "#### Few-Shot Prompting\n",
        "- Few-shot prompting is a technique used with large language models (LLMs) where the prompt includes a few examples or demonstrations of the task you want the model to perform.\n",
        "- This approach helps the model understand the task better and improve its performance by providing context and patterns to follow.\n",
        "- Few-shot prompting involves providing a few examples within the prompt.These examples demonstrate the task or desired output format, helping the model learn from the provided context.\n",
        "- By including a small number of examples, the model can better understand the task and produce more accurate and relevant outputs.\n",
        "This method leverages the model's ability to generalize from the examples to new, unseen inputs.\n",
        "- Few-shot prompting strikes a balance between zero-shot prompting (no examples) and full fine-tuning (many examples).\n",
        "It allows for improved performance without the need for extensive training data or fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a4ea459-28a3-4764-8858-c26601f2e165",
      "metadata": {
        "id": "0a4ea459-28a3-4764-8858-c26601f2e165",
        "outputId": "65370589-1fb4-432d-cb64-1fc8f75a24e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "J'adore programmer.\n"
          ]
        }
      ],
      "source": [
        "prompt = '''Translate the following English sentences to French:\n",
        "1. English: Hello, how are you?\n",
        "   French: Bonjour, comment ça va?\n",
        "2. English: Good morning.\n",
        "   French: Bonjour.\n",
        "3. English: I love programming.\n",
        "   French:\n",
        "'''\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion( messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6478b07a-0821-4544-b45d-1f3e54844a69",
      "metadata": {
        "id": "6478b07a-0821-4544-b45d-1f3e54844a69",
        "outputId": "bee5bb67-87e8-4959-803e-2bf13340debc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The answer is True.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
        "A: The answer is False.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
        "A: The answer is True.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
        "A: The answer is True.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
        "A: The answer is False.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
        "A:\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "response = get_completion( messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b562f596-703f-4542-a179-a847f2c5aed5",
      "metadata": {
        "id": "b562f596-703f-4542-a179-a847f2c5aed5",
        "outputId": "bb4feb29-95d0-4bca-bab5-7874f321e78e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Negative\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"This is awesome! // Negative\n",
        "This is bad! // Positive\n",
        "Wow that movie was rad! // Positive\n",
        "What a horrible show! //\"\"\"\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "response = get_completion( messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40fb4e3c-b8b7-4c69-b116-f3374af80da2",
      "metadata": {
        "id": "40fb4e3c-b8b7-4c69-b116-f3374af80da2"
      },
      "source": [
        "<hr style=\"border: 2px solid black; margin: 5px 0;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d211d531-8c0e-451b-bc40-4de9e192ccaa",
      "metadata": {
        "id": "d211d531-8c0e-451b-bc40-4de9e192ccaa"
      },
      "source": [
        "#### Chain-of-Thought Prompting\n",
        "- Chain of Thought (CoT) prompting is a technique used with large language models (LLMs) to improve their reasoning capabilities and task performance.\n",
        "- This method involves breaking down the reasoning process into a series of intermediate steps or logical sequences, guiding the model through a structured thought process.\n",
        "- Chain of Thought prompting involves explicitly outlining the intermediate steps or reasoning process required to arrive at the final answer.\n",
        "- This structured approach helps the model follow a logical sequence and understand complex tasks better.\n",
        "- By providing a clear sequence of thought, the model can more accurately perform tasks that require multi-step reasoning, such as mathematical problem-solving, logical inference, and complex decision-making.\n",
        "- It helps in generating more coherent and accurate outputs by breaking down the task into manageable parts.\n",
        "- The prompt includes a series of steps or questions that lead the model through the thought process.\n",
        "- This guidance helps the model stay on track and produce the desired outcome by following the logical flow.\n",
        "![image.png](attachment:708341a9-ca9b-4645-96f7-f946183b49aa.png)"
      ]
    },
    {
      "cell_type": "raw",
      "id": "39663039-b617-4541-a6d5-92a2950e3c2c",
      "metadata": {
        "id": "39663039-b617-4541-a6d5-92a2950e3c2c"
      },
      "source": [
        "Prompt :\n",
        "\n",
        "Q: If you have 3 apples and you buy 2 more apples, how many apples do you have in total?\n",
        "A: Let's break it down:\n",
        "   - You start with 3 apples.\n",
        "   - You buy 2 more apples.\n",
        "   - Adding these together: 3 + 2 = 5.\n",
        "So, you have 5 apples in total.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "734a849d-4f04-4bd5-833c-f7ba2969c642",
      "metadata": {
        "id": "734a849d-4f04-4bd5-833c-f7ba2969c642",
        "outputId": "52b9a63f-0f21-445c-d360-d2c21f5ac665"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Let's break it down:\n",
            "   - You start with 5 apples.\n",
            "   - You give 1 apple to your friend, leaving you with 4 apples.\n",
            "   - You buy 2 more apples.\n",
            "   - Adding these together: 4 + 2 = 6.\n",
            "   - So, you have 6 apples in total.\n"
          ]
        }
      ],
      "source": [
        "prompt = '''\n",
        "Q: If you have 3 apples and you buy 2 more apples, how many apples do you have in total?\n",
        "A: Let's break it down:\n",
        "   - You start with 3 apples.\n",
        "   - You buy 2 more apples.\n",
        "   - Adding these together: 3 + 2 = 5.\n",
        "   - So, you have 5 apples in total.\n",
        "Q: If you have 5 apples, one given to your friend and you buy 2 more apples, how many apples do you have in total?\n",
        "A: '''\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "response = get_completion( messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04c4754e-7c8f-458a-9387-59873fcb1979",
      "metadata": {
        "id": "04c4754e-7c8f-458a-9387-59873fcb1979"
      },
      "source": [
        "#### Zero-shot COT Prompting\n",
        "![image.png](attachment:e0f4eae1-ddf9-42d5-ad8a-43f2ac4f1a91.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73ae3a43-2cff-4e31-aca8-addbfe37c556",
      "metadata": {
        "id": "73ae3a43-2cff-4e31-aca8-addbfe37c556",
        "outputId": "fb6e8cf2-2569-4d8b-bb30-32242a7f5a70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After giving away 4 apples and eating 1, you would have 10 - 4 - 1 = 5 apples remaining.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman.\n",
        "I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
        "\"\"\"\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "response = get_completion( messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17d40545-9931-4df5-a308-592595700814",
      "metadata": {
        "id": "17d40545-9931-4df5-a308-592595700814",
        "outputId": "6694571e-446a-40c4-aa75-917fbf82236d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You started with 10 apples.\n",
            "You gave away 2 to the neighbor and 2 to the repairman, so you have 10 - 2 - 2 = 6 apples left.\n",
            "You then bought 5 more apples, so you now have 6 + 5 = 11 apples.\n",
            "After eating 1 apple, you have 11 - 1 = 10 apples remaining.\n",
            "\n",
            "So, you remained with 10 apples.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman.\n",
        "I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
        "Let's think step by step.\"\"\"\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "response = get_completion( messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42acf8ed-dc1f-437d-baca-2202b3868235",
      "metadata": {
        "id": "42acf8ed-dc1f-437d-baca-2202b3868235"
      },
      "source": [
        "<hr style=\"border: 2px solid black; margin: 5px 0;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a45108a-3154-4a0c-8e68-626ef04d41b4",
      "metadata": {
        "id": "5a45108a-3154-4a0c-8e68-626ef04d41b4"
      },
      "source": [
        "#### Self-Consistency Prompting\n",
        "- Self-Consistency Prompting is a technique used with large language models (LLMs) to enhance their reliability and accuracy by generating multiple responses to a prompt and selecting the most consistent answer.\n",
        "- The model generates several responses to the same prompt instead of just one.\n",
        "- This helps capture a range of possible answers, considering different reasoning paths the model might take.\n",
        "- The responses are evaluated for consistency.\n",
        "- The most consistent response across the generated set is selected as the final answer, assuming that the correct reasoning is likely to appear multiple times.\n",
        "- By selecting the most consistent answer, self-consistency prompting reduces the impact of outliers or incorrect reasoning paths.\n",
        "- This approach enhances the reliability and accuracy of the model’s output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72eab366-b0c6-4c60-a2fc-3e8b5f141f1f",
      "metadata": {
        "scrolled": true,
        "id": "72eab366-b0c6-4c60-a2fc-3e8b5f141f1f",
        "outputId": "895c4933-1de0-448b-e753-a41c0d20b178"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Responses:\n",
            "Tokyo\n",
            "The capital of Japan is Tokyo.\n",
            "Tokyo\n",
            "Tokyo\n",
            "The capital of Japan is Tokyo.\n",
            "\n",
            "Final Answer: Tokyo\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Hypothetical function to generate responses\n",
        "def generate_responses(prompt, num_responses=5):\n",
        "    responses = []\n",
        "    for _ in range(num_responses):\n",
        "        messages = [\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": prompt\n",
        "                }\n",
        "            ]\n",
        "        response = get_completion( messages)\n",
        "        responses.append(response)\n",
        "    return responses\n",
        "\n",
        "# Function to determine the most consistent answer\n",
        "def self_consistency_prompting(prompt, num_responses=5):\n",
        "    responses = generate_responses(prompt, num_responses)\n",
        "    print(\"Generated Responses:\")\n",
        "    for response in responses:\n",
        "        print(response)\n",
        "\n",
        "    # Count the frequency of each response\n",
        "    response_counts = Counter(responses)\n",
        "\n",
        "    # Select the most common response\n",
        "    most_common_response = response_counts.most_common(1)[0][0]\n",
        "\n",
        "    return most_common_response\n",
        "\n",
        "# Example usage\n",
        "prompt = \"Q: What is the capital of Japan?\\nA:\"\n",
        "final_answer = self_consistency_prompting(prompt)\n",
        "print(\"\\nFinal Answer:\", final_answer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52ddce02-6e5f-4c54-8c5e-41d1e7a242be",
      "metadata": {
        "id": "52ddce02-6e5f-4c54-8c5e-41d1e7a242be"
      },
      "source": [
        "<hr style=\"border: 2px solid black; margin: 5px 0;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dad5054b-73ec-46a1-9437-779efc39c820",
      "metadata": {
        "id": "dad5054b-73ec-46a1-9437-779efc39c820"
      },
      "source": [
        "### Prompt Chaining"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e841141f-e3b9-48f4-a54d-e069067e7201",
      "metadata": {
        "id": "e841141f-e3b9-48f4-a54d-e069067e7201"
      },
      "source": [
        "- **Definition:**\n",
        "  - A technique where the output of one prompt is used as the input for the next.\n",
        "  - Useful for breaking down complex tasks into manageable steps.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Applications**\n",
        "- **Data Transformation:**\n",
        "  - Parse, clean, or reformat data iteratively.\n",
        "- **Complex Problem Solving:**\n",
        "  - Decompose large problems into smaller sub-problems.\n",
        "- **Multi-Step Workflows:**\n",
        "  - Tasks like content generation, question answering, or logical reasoning.\n",
        "- **Code Generation:**\n",
        "  - Break coding into steps like outlining, writing, debugging, and explaining.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Structure**\n",
        "1. **Step 1: Initial Input**\n",
        "   - Begin with a query or raw data.\n",
        "2. **Step 2: Process Output**\n",
        "   - Refine or transform the output in subsequent steps.\n",
        "3. **Step 3: Chain Further**\n",
        "   - Use intermediate results for additional tasks until the goal is achieved.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Examples**\n",
        "\n",
        "#### **Example 1: Story Generation**\n",
        "- **Step 1: Generate an Outline**\n",
        "  - *Prompt:* \"Create a detailed outline for a mystery story set in a small town.\"\n",
        "  - *Output:* Outline with characters, setting, and key events.\n",
        "- **Step 2: Write the Opening Scene**\n",
        "  - *Prompt:* \"Write the opening scene based on this outline: [Insert outline].\"\n",
        "- **Step 3: Expand on Key Events**\n",
        "  - *Prompt:* \"Expand the plot twist where the detective uncovers a family secret.\"\n",
        "\n",
        "---\n",
        "\n",
        "#### **Example 2: Data Analysis**\n",
        "- **Step 1: Extract Key Information**\n",
        "  - *Prompt:* \"Extract the names, dates, and events from this text: [Insert text].\"\n",
        "- **Step 2: Summarize Key Findings**\n",
        "  - *Prompt:* \"Summarize the extracted data in a tabular format.\"\n",
        "- **Step 3: Generate Insights**\n",
        "  - *Prompt:* \"Based on this summary, provide three actionable insights.\"\n",
        "\n",
        "---\n",
        "\n",
        "#### **Example 3: Code Debugging**\n",
        "- **Step 1: Analyze Code**\n",
        "  - *Prompt:* \"Analyze the following Python code for errors: [Insert code].\"\n",
        "  - *Output:* Description of errors.\n",
        "- **Step 2: Fix Errors**\n",
        "  - *Prompt:* \"Fix the identified errors in this code and explain the changes.\"\n",
        "- **Step 3: Add Documentation**\n",
        "  - *Prompt:* \"Write comments and a docstring for the corrected code.\"\n",
        "\n",
        "<hr style=\"border: 2px solid black; margin: 5px 0;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dc93d33-3aaf-47fa-9965-b142a53a5aa7",
      "metadata": {
        "id": "6dc93d33-3aaf-47fa-9965-b142a53a5aa7"
      },
      "source": [
        "### **Tree of Thoughts (ToT)**\n",
        "\n",
        "- **Definition:**\n",
        "  - A reasoning framework that generates multiple thought branches to explore potential solutions.\n",
        "  - Enhances the problem-solving capability of LLMs by iteratively evaluating and expanding on ideas.\n",
        "\n",
        "![image.png](attachment:7dd8d972-3090-40f9-880b-bf9e5cff5077.png)\n",
        "---\n",
        "\n",
        "#### **Applications**\n",
        "- **Decision Making:**\n",
        "  - Evaluate multiple options and select the best course of action.\n",
        "- **Creative Writing:**\n",
        "  - Explore diverse plotlines, themes, or character arcs.\n",
        "- **Scientific Reasoning:**\n",
        "  - Formulate hypotheses and test logical deductions.\n",
        "- **Code Optimization:**\n",
        "  - Generate, evaluate, and refine coding solutions.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Structure**\n",
        "1. **Step 1: Generate Initial Thoughts**\n",
        "   - Brainstorm multiple initial solutions or ideas.\n",
        "2. **Step 2: Expand Thoughts**\n",
        "   - Explore each initial thought further by generating sub-thoughts or alternatives.\n",
        "3. **Step 3: Evaluate and Prune**\n",
        "   - Assess the generated thoughts for relevance, feasibility, or quality.\n",
        "   - Retain the most promising thoughts and discard the rest.\n",
        "4. **Step 4: Iterate**\n",
        "   - Continue expanding and evaluating thoughts until the best solution is achieved.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Examples:**\n",
        "\n",
        "#### **Example 1: Decision Making**\n",
        "- **Step 1: Generate Initial Options**\n",
        "  - *Prompt:* \"List three ways to reduce energy consumption in an office.\"\n",
        "  - *Output:* Install motion sensors, encourage remote work, upgrade equipment.\n",
        "- **Step 2: Expand Each Option**\n",
        "  - *Prompt:* \"Detail how motion sensors can save energy in the office.\"\n",
        "- **Step 3: Evaluate and Select**\n",
        "  - *Prompt:* \"Compare the cost and impact of these three options and recommend the best one.\"\n",
        "\n",
        "---\n",
        "\n",
        "#### **Example 2: Creative Writing**\n",
        "- **Step 1: Brainstorm Plot Ideas**\n",
        "  - *Prompt:* \"Suggest three possible endings for this story.\"\n",
        "  - *Output:* Hero wins, bittersweet ending, villain prevails.\n",
        "- **Step 2: Expand Each Idea**\n",
        "  - *Prompt:* \"Describe the events leading to the hero's victory.\"\n",
        "- **Step 3: Evaluate and Finalize**\n",
        "  - *Prompt:* \"Which ending best aligns with the story's themes and character arcs?\"\n",
        "\n",
        "---\n",
        "\n",
        "#### **Example 3: Code Optimization**\n",
        "- **Step 1: Generate Initial Solutions**\n",
        "  - *Prompt:* \"Propose three methods to optimize this Python code snippet.\"\n",
        "- **Step 2: Explore Each Method**\n",
        "  - *Prompt:* \"Explain how using a generator can improve memory efficiency.\"\n",
        "- **Step 3: Test and Evaluate**\n",
        "  - *Prompt:* \"Compare the runtime of these three optimization methods.\"\n",
        "\n",
        "<hr style=\"border: 2px solid black; margin: 5px 0;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3252193c-8855-44dd-b070-f2bc804413c4",
      "metadata": {
        "id": "3252193c-8855-44dd-b070-f2bc804413c4"
      },
      "source": [
        "### **React Prompting**\n",
        "\n",
        "- **Definition:**\n",
        "  - A framework for structured problem-solving where the model alternates between **reasoning** and **acting**.\n",
        "  - Proposed to enable language models to reason through tasks before taking action, ensuring better decision-making and output quality.\n",
        "\n",
        "#### **Key Features**\n",
        "- **Reasoning:**  \n",
        "  - The model generates explanations or logical steps to approach a problem.\n",
        "- **Acting:**  \n",
        "  - Based on the reasoning, the model performs a specific task or action.\n",
        "- **Interleaved Process:**  \n",
        "  - Encourages the model to iteratively reason, act, and refine, improving accuracy and depth of the solution.\n",
        "- **Self-Correction:**  \n",
        "  - Enables the model to analyze its own reasoning and actions, identifying and correcting errors dynamically.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Applications**\n",
        "- **Complex Question Answering:**  \n",
        "  - Iteratively breaks down questions, gathers context, and answers step-by-step.\n",
        "- **Code Debugging:**  \n",
        "  - Combines reasoning about code logic with actionable steps to correct errors.\n",
        "- **Multi-Step Decision Making:**  \n",
        "  - Structures decisions by explaining potential outcomes before choosing an action.\n",
        "- **Interactive Agents:**  \n",
        "  - Implements conversational agents that reason through user queries and provide dynamic responses.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Structure**\n",
        "1. **Step 1: Reasoning**\n",
        "   - The model explains the current situation or problem and proposes a plan of action.\n",
        "2. **Step 2: Acting**\n",
        "   - The model executes an action based on the reasoning.\n",
        "3. **Step 3: Evaluate**\n",
        "   - Assess whether the action resolved the problem or requires refinement.\n",
        "4. **Step 4: Iterate**\n",
        "   - Repeat the cycle of reasoning and acting until the task is complete or the solution is optimal.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Examples:**\n",
        "\n",
        "#### **Example 1: Question Answering**\n",
        "- **Step 1: Reasoning**\n",
        "  - *Prompt:* \"Explain how to calculate the area of a triangle given base and height.\"\n",
        "  - *Output:* \"The area is calculated as (base × height) ÷ 2.\"\n",
        "- **Step 2: Acting**\n",
        "  - *Prompt:* \"Calculate the area of a triangle with base=10 and height=5.\"\n",
        "  - *Output:* \"The area is 25.\"\n",
        "\n",
        "---\n",
        "\n",
        "#### **Example 2: Code Debugging**\n",
        "- **Step 1: Reasoning**\n",
        "  - *Prompt:* \"Analyze the following Python code for logical errors.\"\n",
        "  - *Output:* \"The loop condition causes an infinite loop because the counter isn't updated.\"\n",
        "- **Step 2: Acting**\n",
        "  - *Prompt:* \"Fix the error and rewrite the loop with proper counter updates.\"\n",
        "- **Step 3: Evaluate**\n",
        "  - *Prompt:* \"Test the corrected code to ensure functionality.\"\n",
        "\n",
        "---\n",
        "\n",
        "#### **Example 3: Multi-Step Decision Making**\n",
        "- **Step 1: Reasoning**\n",
        "  - *Prompt:* \"List the pros and cons of using solar panels for home energy.\"\n",
        "  - *Output:* \"Pros: Renewable energy, cost savings. Cons: High initial cost, weather dependency.\"\n",
        "- **Step 2: Acting**\n",
        "  - *Prompt:* \"Given these points, recommend whether to install solar panels in a sunny region.\"\n",
        "  - *Output:* \"Recommendation: Install solar panels for long-term benefits.\"\n",
        "\n",
        "---\n",
        "\n",
        "#### **Benefits**\n",
        "- Combines **logical reasoning** with actionable steps.\n",
        "- Encourages **self-correction**, improving the quality of outputs.\n",
        "- Suitable for tasks requiring **dynamic interaction** and **step-by-step problem-solving**.\n",
        "\n",
        "<hr style=\"border: 2px solid black; margin: 5px 0;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5361a983-045f-41ab-badb-232e460a78c2",
      "metadata": {
        "id": "5361a983-045f-41ab-badb-232e460a78c2"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ed1b3e2-dd4e-4f9d-ab40-31fadbdb2a0a",
      "metadata": {
        "id": "0ed1b3e2-dd4e-4f9d-ab40-31fadbdb2a0a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}